{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from mss import mss\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import keyboard\n",
    "import time\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf                                                               \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        #This is the actual Neural net\n",
    "        model = Sequential([ \n",
    "            Conv2D(32, (8,8), input_shape=(76, 384, 4),\n",
    "                   strides=(2,2), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(5,5), strides=(2, 2)),\n",
    "            Conv2D(64, (4,4), activation='relu', strides=(1,1)),\n",
    "            MaxPooling2D(pool_size=(7, 7), strides=(3, 3)),\n",
    "            Conv2D(128, (1, 1), strides=(1,1), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(3,3), strides=(3,3)),\n",
    "            Flatten(),\n",
    "            Dense(384, activation='relu'),\n",
    "            Dense(64, activation=\"relu\", name=\"layer1\"),\n",
    "            Dense(8, activation=\"relu\", name=\"layer2\"),\n",
    "            Dense(3, activation=\"linear\", name=\"layer3\"),\n",
    "        ])\n",
    "        #pick your learning rate here\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001)) \n",
    "        #This is where you import your pretrained weights\n",
    "        model.load_weights(r\"C:\\Users\\Chakradhar\\Desktop\\DinoGameSpeed4.h5.txt\")\n",
    "        self.model = model\n",
    "        self.memory = []\n",
    "        self.xTrain = []\n",
    "        self.yTrain = []\n",
    "        self.loss = []\n",
    "        self.location = 0\n",
    "\n",
    "\n",
    "    def predict(self, state):\n",
    "        stateConv = state\n",
    "        qval = self.model.predict(np.reshape(stateConv, (1, 76, 384, 4)))\n",
    "        return qval\n",
    "\n",
    "    def act(self, state):\n",
    "        qval = self.predict(state)\n",
    "        #Epsilon-Greedy actions->\n",
    "        z = np.random.random()\n",
    "        epsilon = 0.004\n",
    "        if self.location > 1000:\n",
    "            epsilon = 0.05\n",
    "        epsilon = 0\n",
    "        if z > epsilon:\n",
    "            return np.argmax(qval.flatten())\n",
    "        else:\n",
    "            return np.random.choice(range(3))\n",
    "        return action\n",
    "\n",
    "    # This function stores experiences in the experience replay\n",
    "    def remember(self, state, nextState, action, reward, done, location):\n",
    "        self.location = location\n",
    "        self.memory.append(np.array([state, nextState, action, reward, done]))\n",
    "\n",
    "    #This is where the AI learns\n",
    "    def learn(self):\n",
    "        self.batchSize = 256 \n",
    "        if len(self.memory) > 35000:\n",
    "            self.memory = []\n",
    "            print(\"trimming memory\")\n",
    "        if len(self.memory) < self.batchSize:\n",
    "            print(\"too little info\")\n",
    "            return  \n",
    "        batch = random.sample(self.memory, self.batchSize)\n",
    "        self.learnBatch(batch)\n",
    "\n",
    "    #The alpha value determines how future oriented the AI is.\n",
    "    def learnBatch(self, batch, alpha=0.9):\n",
    "        batch = np.array(batch)\n",
    "        actions = batch[:, 2].reshape(self.batchSize).tolist()\n",
    "        rewards = batch[:, 3].reshape(self.batchSize).tolist()\n",
    "\n",
    "        stateToPredict = batch[:, 0].reshape(self.batchSize).tolist()\n",
    "        nextStateToPredict = batch[:, 1].reshape(self.batchSize).tolist()\n",
    "\n",
    "        statePrediction = self.model.predict(np.reshape(\n",
    "            stateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        nextStatePrediction = self.model.predict(np.reshape(\n",
    "            nextStateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        statePrediction = np.array(statePrediction)\n",
    "        nextStatePrediction = np.array(nextStatePrediction)\n",
    "\n",
    "        for i in range(self.batchSize):\n",
    "            action = actions[i]\n",
    "            reward = rewards[i]\n",
    "            nextState = nextStatePrediction[i]\n",
    "            qval = statePrediction[i, action]\n",
    "            if reward < -5: \n",
    "                statePrediction[i, action] = reward\n",
    "            else:\n",
    "                #this is the q learning update rule\n",
    "                statePrediction[i, action] += alpha * (reward + 0.95 * np.max(nextState) - qval)\n",
    "\n",
    "        self.xTrain.append(np.reshape(\n",
    "            stateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        self.yTrain.append(statePrediction)\n",
    "        history = self.model.fit(\n",
    "            self.xTrain, self.yTrain, batch_size=5, epochs=1, verbose=0)\n",
    "        loss = history.history.get(\"loss\")[0]\n",
    "        print(\"LOSS: \", loss)\n",
    "        self.loss.append(loss)\n",
    "        self.xTrain = []\n",
    "        self.yTrain = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Enviornment:\n",
    "    def __init__(self):\n",
    "        self.mon = {'top': 243, 'left': 0, 'width': 1366, 'height': 270} # 720p resolution        \n",
    "        self.sct = mss()\n",
    "        self.counter = 0\n",
    "        self.startTime = -1\n",
    "        self.imageBank = []\n",
    "        self.imageBankLength = 4 #number of frames for the conv net\n",
    "        self.actionMemory = 2 #init as 2 to show no action taken   \n",
    "        #image processing\n",
    "        self.ones = np.ones((76,384,4))\n",
    "        self.zeros = np.zeros((76,384,4))  \n",
    "        self.zeros1 = np.zeros((76,384,4))\n",
    "        self.zeros2 = np.zeros((76,384,4))\n",
    "        self.zeros3 = np.zeros((76,384,4))\n",
    "        self.zeros4 = np.zeros((76,384,4))\n",
    "        self.zeros1[:,:,0] = 1\n",
    "        self.zeros2[:,:,1] = 1\n",
    "        self.zeros3[:,:,2] = 1\n",
    "        self.zeros4[:,:,3] = 1\n",
    "\n",
    "    def startGame(self):\n",
    "        #start the game, giving the user a few seconds to click on the chrome tab after starting the code\n",
    "        for i in reversed(range(3)):\n",
    "            print(\"game starting in \", i)\n",
    "            time.sleep(1)\n",
    "\n",
    "    def step(self, action):        \n",
    "        actions ={\n",
    "            0: 'space',\n",
    "            1: 'down'\n",
    "        }            \n",
    "        if action != self.actionMemory:\n",
    "            if self.actionMemory != 2:\n",
    "                keyboard.release(actions.get(self.actionMemory))\n",
    "            if action != 2:\n",
    "                keyboard.press(actions.get(action))\n",
    "        self.actionMemory = action\n",
    "\n",
    "        #This is where the screenshot happens\n",
    "        screenshot = self.sct.grab(self.mon)\n",
    "        img = np.array(screenshot)[:, :, 0]\n",
    "        processedImg = self._processImg(img)\n",
    "        state = self._imageBankHandler(processedImg)\n",
    "        done = self._done(processedImg)\n",
    "        reward = self._getReward(done)\n",
    "        return state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.startTime = time.time()\n",
    "        keyboard.press(\"space\")\n",
    "        time.sleep(0.5)\n",
    "        keyboard.release(\"space\")\n",
    "        return self.step(0)\n",
    "\n",
    "    def _processImg(self, img):\n",
    "        img = Image.fromarray(img)\n",
    "        img = img.resize((384, 76), Image.ANTIALIAS)\n",
    "        if np.sum(img) > 2000000:\n",
    "            img = ImageOps.invert(img)\n",
    "        img = self._contrast(img)\n",
    "        img = np.reshape(img, (76,384))\n",
    "        return img\n",
    "\n",
    "    def _contrast(self,pixvals):\n",
    "        minval = 32 \n",
    "        maxval = 171\n",
    "        pixvals = np.clip(pixvals, minval, maxval)\n",
    "        pixvals = ((pixvals - minval) / (maxval - minval))\n",
    "        return pixvals\n",
    "\n",
    "    def _imageBankHandler(self, img):\n",
    "        img = np.array(img)\n",
    "        while len(self.imageBank) < (self.imageBankLength): \n",
    "            self.imageBank.append(np.reshape(img,(76,384,1)) * self.ones)\n",
    "\n",
    "        \n",
    "        bank = np.array(self.imageBank)\n",
    "        toReturn = self.zeros\n",
    "        img1 = (np.reshape(img,(76,384,1)) * self.ones)  * self.zeros1\n",
    "        img2 = bank[0] * self.zeros2\n",
    "        img3 = bank[1] * self.zeros3\n",
    "        img4 = bank[2] * self.zeros4\n",
    "\n",
    "\n",
    "        toReturn = np.array(img1 + img2 + img3 + img4)        \n",
    "\n",
    "        self.imageBank.pop(0)\n",
    "        self.imageBank.append(np.reshape(img,(76 ,384,1)) * self.ones)\n",
    "\n",
    "        return toReturn\n",
    "\n",
    "    def _getReward(self,done):\n",
    "        if done:\n",
    "            return -15\n",
    "        else: \n",
    "            return 1\n",
    "            return time.time() - self.startTime\n",
    "        \n",
    "    def _done(self,img):\n",
    "        img = np.array(img)\n",
    "        img  = img[30:50, 180:203]\n",
    "\n",
    "\n",
    "        val = np.sum(img)\n",
    "        #Sum of the reset pixels when the game ends in the night mode\n",
    "        expectedVal = 331.9352517985612 \n",
    "        #Sum of the reset pixels when the game ends in the day mode\n",
    "        expectedVal2 = 243.53\n",
    "\n",
    "        # This method checks if the game is done by reading the pixel values\n",
    "        # of the area of the screen at the reset button. Then it compares it to\n",
    "        # a pre determined sum. You might need to fine tune these values since each\n",
    "        # person's viewport will be different. use the following print statements to \n",
    "        # help you find the appropirate values for your use case \n",
    "\n",
    "        if np.absolute(val-expectedVal) > 15 and np.absolute(val-expectedVal2) > 15: #seems to work well\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotX = []\n",
    "while True:\n",
    "    agent = Agent() #currently agent is configured with only 2 actions\n",
    "    env = Enviornment()\n",
    "    env.startGame()    \n",
    "    for i in tqdm(range(3500)): \n",
    "        state, reward, done = env.reset()\n",
    "        epReward = 0\n",
    "        done = False\n",
    "        episodeTime = time.time()\n",
    "        stepCounter = 0\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            nextState, reward, done = env.step(action)\n",
    "            #This next section is storing more memory of later parts of the game since \n",
    "            #if you don't do this, most of the experience replay fills up with the \n",
    "            if stepCounter> 700:\n",
    "                for _ in range(5):\n",
    "                    agent.remember(state, nextState, action, reward, done, stepCounter)\n",
    "            elif stepCounter> 40:\n",
    "                agent.remember(state, nextState, action, reward, done, stepCounter)                \n",
    "            if done == True: #game ended\n",
    "                for _ in range(10):\n",
    "                    agent.remember(state, nextState, action, reward, done, stepCounter)\n",
    "                print(\"breaking\")\n",
    "                break\n",
    "            state = nextState\n",
    "            stepCounter += 1\n",
    "            epReward += reward\n",
    "\n",
    "        #post episode\n",
    "        if stepCounter != 0:\n",
    "            print(\"Avg Frame-Rate: \", 1/((time.time()-episodeTime)/stepCounter))\n",
    "        plotX.append(epReward)\n",
    "        print(epReward)\n",
    "        agent.learn()\n",
    "\n",
    "\n",
    "       \n",
    "        if i % 20 == 0:\n",
    "            agent.model.save_weights (r\"C:\\Users\\Chakradhar\\Desktop\\DinoGameSpeed4.h5.txt\")\n",
    "            print( \"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
